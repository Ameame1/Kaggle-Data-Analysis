---
title: "CITS4009 - Project 2"
author: "Ame Liu(22910358),Hanzhe Yang(22672987)"
output: html_notebook
---
### Introduction
The data set analyzed can be obtained from the Kaggle platform. https://www.kaggle.com/silicon99/dft-accident-data

This dataset comes from the UK police forces who collect data on every vehicle collision in the UK from 2005 to 2015.In this project only one csv file named Accidents0515.csv which includes part of information is conisdered.

This project consists of four parts.After preparation,  part two is a warmup part, it will focus on the frequency of accidents during the year and the characteristics to start the anaysis. In the third part, it will select three factors to investigate in order to get what influence these factors have on the accident occurence and the accident severtity. Finally, a conclusion will be given

### [1]Data preparation

#### (1)Load libraries:
```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)
library(hexbin)
library(party)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(skimr)
library(DataExplorer)
library(caret)
library(pROC)
library(tidyverse)
library(randomForest)
library(e1071)
library(fpc)


```
#### (2)Load the original database:
```{r}
accident <-read.csv("/Users/ame/Documents/R File/Accidents0515.csv")
```
#### (3)overall database data
Analyze the data with str( ):
```{r}
str(accident)
```
There are 1780653 obs and 32 variables consisting of 5 char, 25 int and 2 numeric.

Analyze the data with summary( ):
```{r}
summary(accident)
```
The amount of data is very large.

Analyze the data with head( ):
```{r}
head(accident)
```
Count the number of NA for each classification:
```{r}
apply(is.na(accident), 2, sum)
```
Numerical and integer variables are selected as the independent variables of the research object:
We first use the decision tree model for a general browse, and then determine the specific multi classification. 
After that, we use other models respectively. We classify the following steps as data preprocessing.
#### (4)Select the appropriate dependent variable
```{r}
#1.Accident_Severity
#2.Number_of_Vehicles
#3.Number_of_Casualties
#4.Road_Type
#5.Speed_limit
#6.Junction_Detail
#7.Junction_Control
#8.Pedestrian_Crossing.Human_Control
#9.Pedestrian_Crossing
#10.Light_Conditions
#11.Weather_Conditions
#12.Road_Surface_Conditions
#13.Special_Conditions_at_Site
#14.Carriageway_Hazards
#15.Urban_or_Rural_Area
data1<- subset(accident,select=c(Accident_Severity,Number_of_Vehicles,Number_of_Casualties,Road_Type,Speed_limit,Junction_Detail,Junction_Control,Pedestrian_Crossing.Human_Control,Pedestrian_Crossing.Human_Control,Pedestrian_Crossing.Physical_Facilities,Light_Conditions,Weather_Conditions,Road_Surface_Conditions,Special_Conditions_at_Site,Carriageway_Hazards,Urban_or_Rural_Area))
data1 <- na.omit(data1)              
plot_missing(data1)     #Show missing values
data1$Accident_Severity<- factor(data1$Accident_Severity)     #Convert numeric category to factor category
skim(data1)            #Skim data after data processing
table(data1$Accident_Severity)    #Distribution of dependent variables
data1$Accident_Severity <- fct_collapse(data1$Accident_Severity,"1"=c("2"),"2"=c("3"))      #Merge factor
data1       #After data processing        
set.seed(35)
#The set. Seed() function is used to ensure that the random numbers you randomly generate are consistent
trains <- createDataPartition(
  y= data1$Accident_Severity,
  p=0.7,
  list= F
  
)
traindata <- data1[trains,]     #70% training data      
testdata <- data1[-trains,]     #30% testing data
#Distribution of dependent variables after splitting
table(traindata$Accident_Severity)   
table(testdata$Accident_Severity)   
colnames(data1)

#Dependent variable and independent variable construction formula:
form_clsm <- as.formula(
  paste0(
    "Accident_Severity ~ ",
    paste(colnames(traindata)[2:16],collapse = " + ")
  )
)
#Building decision tree model
set.seed(35)
fit_dt_clsm <- rpart(
  form_clsm,
  data = traindata,
  method = "anova",
  parms = list(split = "information"),
  control = rpart.control(cp=0.005) 
  )
#Complexity related data
printcp(fit_dt_clsm)
plotcp(fit_dt_clsm)
#post-pruning
fit_dt_clsm_pruned <- prune(fit_dt_clsm, cp = 0.0016)
#Variable importance
fit_dt_clsm_pruned$variable.importance
varimpdata <- data.frame(importance = fit_dt_clsm_pruned$variable.importance)
varimpdata 
#Drawing
ggplot(varimpdata,aes(x = as_factor(rownames(varimpdata)),y= importance))+geom_col() + labs(x= "variables")+ theme(axis.text.x = element_text(angle = 15,hjust = 1))+ theme_classic()

```
We select the first four factors that most affect the dependent variables as the research objects of the next three models, and up to now, all data preprocessing has been completed.
### [2]Classification
#### (1)Decision_tree Model
We chose accident_ Severity,Number_ of_ Vehicles,Speed_ limit,Urban_ or_ Rural_ Area,Light_ Conditions is used as a new data set to build different model research data sets.
```{r}
Decision_tree<- subset(accident,select=c(Accident_Severity,Number_of_Vehicles,Speed_limit,Urban_or_Rural_Area,Light_Conditions))
Decision_tree <- na.omit(Decision_tree)
skim(Decision_tree)
plot_missing(Decision_tree)
#Variable type correction
Decision_tree$Accident_Severity<- factor(Decision_tree$Accident_Severity)
Decision_tree$Accident_Severity <- fct_collapse(Decision_tree$Accident_Severity,"1"=c("2"))
Decision_tree$Accident_Severity <- fct_collapse(Decision_tree$Accident_Severity,"2"=c("3"))
#Skim view after data processing
skim(Decision_tree)
Decision_tree


#########################
#Split data
set.seed(35)
trains <- createDataPartition(
  y= data1$Accident_Severity,
  p=0.7,
  list= F
  
)
traindata <- Decision_tree[trains,]
testdata <- Decision_tree[-trains,]
hist(traindata$Number_of_Vehicles,breaks = 50)
hist(traindata$Speed_limit,breaks = 50)
hist(traindata$Urban_or_Rural_Area,breaks = 50)
hist(traindata$Light_Conditions,breaks = 50)
##############################
#Dependent variable and independent variable construction formula
colnames(Decision_tree)
form_clsm <- as.formula(
  paste0(
    "Accident_Severity ~ ",
    paste(colnames(traindata)[2:5],collapse =  " + ")
  )
)
form_clsm

#Training model
set.seed(35)
fit_dt_clsm<- rpart(
  form_clsm,
  data = traindata,
  method = "anova",
  parms = list(split = "information"),
  control  = rpart.control(cp = 0.001)
)
#Primitive regression tree
fit_dt_clsm
#Complexity related data
printcp(fit_dt_clsm)
plotcp(fit_dt_clsm)
#post-pruning
fit_dt_clsm_pruned <- prune(fit_dt_clsm, cp = 0.0012)
print(fit_dt_clsm_pruned)

#Variable importance value
fit_dt_clsm_pruned$variable.Accident_Severity
#Variable importance diagram
varimpdata <- 
  data.frame(importance = fit_dt_clsm_pruned$variable.importance)
ggplot(varimpdata,
       aes(x = as_factor(rownames(varimpdata)), y = importance)) +
  geom_col()+
  labs(x = "variables")+theme(axis.test.x = element_text(angle = 15,hjust=1))+
  theme_classic()

#Tree diagram
prp(fit_dt_clsm_pruned,
      type = 1,
      extra = 101,
      fallen.leaves = TRUE,
      main= "Decision Tree")
####################################

#Forecast
#Training set prediction results
trainpred <- predict(fit_dt_clsm_pruned, newdata = traindata)
#Prediction error index of training set
train <- data.frame(obs = traindata$Accident_Severity, pred = trainpred)

#Figure training set prediction results
plot(x = traindata$Accident_Severity,
       y = trainpred,
       xlab = "Actual",
       ylab = "Predication",
       main = "Compare",
       sub = "Train")
trainlinmod <- lm(trainpred ~ traindata$Accident_Severity)
abline(trainlinmod, col = "blue", lwd = 2.5, lty = "solid")
abline(a = 0, b=1, col = "red", lwd = 2.5, lty = "dashed")
legend("topleft",
            legend = c("Model", "Base"),
            col = c("blue" , "red") ,
            lwd = 2.5,
            lty = c("solid", "dashed"))

#Test set prediction results
testpred <- predict(fit_dt_clsm_pruned, newdata = testdata)

#Test set prediction error index
test <- data.frame(obs = testdata$Accident_Severity, pred = testpred)

#Graphical test set prediction results
plot(x = testdata$Accident_Severity,
       y = testpred,
       xlab = "Actual",
       ylab = "Predication",
       main = "Compare",
       sub = "Train")
testlinmod <- lm(testpred ~ testdata$Accident_Severity)
abline(testlinmod, col = "blue", lwd = 2.5, lty = "solid")
abline(a = 0, b=1, col = "red", lwd = 2.5, lty = "dashed")
legend("topleft",
            legend = c("Model", "Base"),
            col = c("blue" , "red") ,
            lwd = 2.5,
            lty = c("solid", "dashed"))

#Centralized display of prediction results of training set and test set
predresult <- 
  data.frame(bos = c(traindata$Accident_Severity, testdata$Accident_Severity),
                    pred = c(trainpred, testpred),
                    group = c(rep("Train", length(trainpred)),
                                    rep("Test", length(testpred))))


ggplot(predresult,
            aes(x = bos , y = pred, fill = group , color = group)) + 
      geom_point(shape = 21, size = 3)+
      geom_smooth(method = "lm", se = F, size = 1.2)+
      geom_abline(intercept = 0, slope = 1, size = 1.2) +
      labs(fill = NULL, colour = NULL) +
      theme(legend.position = "bottom")


```
Decision tree is a basic classification and regression method.
From the decision tree, we can find that the root node is number_ of_ Vehicles, followed by speed_ Limit, and finally urban_ or_ Rural_ Area.
However, because the data are not continuous variables, we find that the model constructed by the decision tree is quite different from the actual value, 
and the prediction success rate is also low, so it is obvious that this model is not a wise choice.

#### (2)Random_Forest Model
```{r}

Random_Forest <- subset(accident,select=c(Accident_Severity,Number_of_Vehicles,Speed_limit,Urban_or_Rural_Area,Light_Conditions))
Random_Forest$Accident_Severity<- factor(Random_Forest$Accident_Severity)
Random_Forest$Accident_Severity <- fct_collapse(Random_Forest$Accident_Severity,"1"=c("2"))
Random_Forest$Accident_Severity <- fct_collapse(Random_Forest$Accident_Severity,"2"=c("3"))
Random_Forest <- na.omit(Random_Forest)
#Split data
set.seed(35)
trains <- createDataPartition(
  y = Random_Forest$Accident_Severity,
  p = 0.7,
  list = F
)

traindata <- Random_Forest[trains,]
testdata <- Random_Forest[-trains,]
#Distribution of dependent variables
table(Random_Forest$Accident_Severity)
#Independent variable dependent variable construction formula
colnames(Random_Forest)
form_clsm <- as.formula(
  paste0(
    "Accident_Severity ~ " ,
    paste(colnames(traindata)[2:5], collapse = "+")
  )
)
form_clsm

#Build model
set.seed(35)
fit_rf_clsm <- randomForest(
  form_clsm,
  data = traindata,
  ntree = 50,
  mrty = 4,
  importance = T
)
fit_rf_clsm
plot(fit_rf_clsm,main = "ERROR & TREES")
legend(
  "top",legend = colnames(fit_rf_clsm$err.rate),
  lty = 1:4,
  col = 1:4,
  horiz = T
)


#Variable importance
importance(fit_rf_clsm)
varImpPlot(fit_rf_clsm,type = 1)
varImpPlot(fit_rf_clsm,type = 2)

#Partial correlation diagram
partialPlot(x = fit_rf_clsm,
            pred.data = traindata,
            x.var = Number_of_Vehicles,
            which.class = "1",
            ylab= "1")

partialPlot(x = fit_rf_clsm,
            pred.data = traindata,
            x.var = Speed_limit ,
            which.class = "1",
            ylab= "1")
partialPlot(x = fit_rf_clsm,
            pred.data = traindata,
            x.var = Urban_or_Rural_Area,
            which.class = "1",
            ylab= "1")
partialPlot(x = fit_rf_clsm,
            pred.data = traindata,
            x.var = Light_Conditions,
            which.class = "1",
            ylab= "1")

#Forecast
#Training set prediction probability
trainpredprob <- predict(fit_rf_clsm, newdata = traindata, type = "prob")
multiclass.roc(response= traindata$Accident_Severity, predictor = trainpredprob)
trainpredlab <- predict(fit_rf_clsm, newdata = traindata,type = "class")
confusionMatrix(data = trainpredlab,
                reference = traindata$Accident_Severity,
                mode = "everything")
RandomForestFinal1<- data.frame(multiClassSummary(
  data.frame(obs = traindata$Accident_Severity,pred = trainpredlab),
  lev = levels(traindata$Accident_Severity)
))
RandomForestFinal1
#Test set prediction probability
testpredprob <- predict(fit_rf_clsm,newdata = testdata, type= "prob")
#Test set ROC
multiclass.roc(response = testdata$Accident_Severity, predictor = testpredprob)
#Test set prediction classification
testpredlab <- predict(fit_rf_clsm, newdata = testdata, type = "class")
#Test set confusion matrix
confusionMatrix(data= testpredlab,
                reference = testdata$Accident_Severity,
                mode = "everything")
#Test set synthesis results
RandomForestFinal2<- data.frame(multiClassSummary(
  data.frame(obs = testdata$Accident_Severity, pred = testpredlab),
  lev = levels(testdata$Accident_Severity)
))
RandomForestFinal2

```
The principle of the Random_Forest model is to integrate many decision trees into a forest and use them together to predict the final result.
We can see random_ The accuracy of training set and test set of forest model is about 85%, which can be regarded as a good ideal model.

#### (3)SVM Model
```{r}
SVM <- subset(accident,select=c(Accident_Severity,Number_of_Vehicles,Speed_limit,Urban_or_Rural_Area,Light_Conditions))
SVM$Accident_Severity<- factor(SVM$Accident_Severity)
SVM$Accident_Severity <- fct_collapse(SVM$Accident_Severity,"1"=c("2"))
SVM$Accident_Severity <- fct_collapse(SVM$Accident_Severity,"2"=c("3"))
SVM <- na.omit(SVM)
#Split data
set.seed(35)
trains <- createDataPartition(
  y = SVM$Accident_Severity,
  p = 0.02,
  list = F
)

traindata <- SVM[trains,]

trains2 <- createDataPartition(
  y = SVM$Accident_Severity,
  p = 0.99,
  list = F
)
testdata <- SVM[-trains2,]
traindata
testdata 


#Distribution of dependent variables
table(SVM$Accident_Severity)
#Independent variable dependent variable construction formula
colnames(SVM)
form_clsm <- as.formula(
  paste0(
    "Accident_Severity ~ " ,
    paste(colnames(traindata)[2:5], collapse = "+")
  )
)
form_clsm

#Build model
fit_svm_clsm <- svm(form_clsm,
                    data = traindata,
                    kernel = "radial",
                    cost = 1,
                    probability = T)
fit_svm_clsm 
#Training set prediction probability
trainpred <- predict(fit_svm_clsm,newdata = traindata,probability = T)
trainpredprob <- attr(trainpred,"probabilities")
#Training set ROC
multiclass.roc(response = traindata$Accident_Severity,predictor = trainpredprob)
#Training set confusion matrix
confusionMatrix(data = trainpred,
                reference = traindata$Accident_Severity,
                mode = "everything")
#Training set synthesis results
SVMFinal1 <- data.frame(multiClassSummary(
  data.frame(obs = traindata$Accident_Severity,pred = trainpred),
  lev = levels(traindata$Accident_Severity)
))
SVMFinal1
#Test set prediction probability
testpred <- predict(fit_svm_clsm,newdata = testdata,probability = T)
testpredprob <- attr(testpred,"probabilities")
#Test set ROC
multiclass.roc(response = testdata$Accident_Severity,predictor = testpredprob)
#Test set confusion matrix
confusionMatrix(data = testpred,
                reference = testdata$Accident_Severity,
                mode = "everything")
#Test set synthesis results
SVMFinal2 <- data.frame(multiClassSummary(
  data.frame(obs = testdata$Accident_Severity,pred = testpred),
  lev = levels(testdata$Accident_Severity)
))
SVMFinal2
```
Support vector machines (SVM) is a binary classification model. 
Its basic model is the linear classifier with the largest interval defined in the feature space. 
The learning algorithm of SVM is the optimization algorithm for solving convex quadratic programming. 
We can find that the prediction probability of training set and test set is more than 85%, which is an ideal model.

### [3]Clustering
In this section, we will use hierarchical clustering and kMeans clustering to do the clustering task. 
Background: there are 51 police organizations in Britain which are in charge of different countries.When an accident occurs, the nearest police organization will handle that accident. We want to use clustering to help police organizations own more reasonable manpower management.The idea is that if a police organization always handles serious accidents, more police constables should be allocated to it.
```{r}
accident <- na.omit(accident)
data_clustering_orignial <- accident 

#convert indicator back into categorical variables, which is more readable.
data_levels<-c(1,3:7,10:14,16,17,20:23,30:37,40:48,50,52:55,60:63,91:98)

data_labels<-c("Metropolitan Police","Cumbria","Lancashire","Merseyside","Greater Manchester","Cheshire","Northumbria","Durham","North Yorkshire","West Yorkshire","South Yorkshire","Humberside","Cleveland","West Midlands","Staffordshire","West Mercia","Warwickshire","Derbyshire","Nottinghamshire","Lincolnshire","Leicestershire","Northamptonshire","Cambridgeshire","Norfolk","Suffolk","Bedfordshire","Hertfordshire","Essex","Thames Valley","Hampshire","Surrey","Kent","Sussex","City of London","Devon and Cornwall","Avon and Somerset","Gloucestershire","Wiltshire","Dorset","North Wales","Gwent","South Wales","Dyfed-Powys","Northern","Grampian","Tayside","Fife","Lothian and Borders","Central","Strathclyde","Dumfries and Galloway")

data_clustering_orignial$Police_Force<-factor(data_clustering_orignial$Police_Force,levels=data_levels,labels=data_labels)


data_clustering_orignial$Police_Force<-as.character(data_clustering_orignial$Police_Force)


#To measure the severity of an accident, number of casualties and vehicles should be considered. What's more, sometimes there could be a fatal accident in which only one casualty, so the severity from the data set should be included here as well.
#It is noted that the larger value of Mean_Accident_Severity a police force has, the severity is lighter.
data_clustering_orignial<- data_clustering_orignial%>% select(Police_Force,Number_of_Vehicles,Number_of_Casualties,Accident_Severity)

data_clustering_groupby_PF <- data_clustering_orignial %>% group_by(Police_Force) %>% summarise(Mean_Number_of_Vehicles = mean(Number_of_Vehicles),Mean_Number_of_Casualties = mean(Number_of_Casualties),Mean_Accident_Severity = mean(Accident_Severity))


data_clustering <- data_clustering_groupby_PF %>% select(-Police_Force)

#We do scaling here to make all the variable be zero-mean and standard variation.
scale(data_clustering)
```

```{r}

#we calculate the euclidean distance of each data point in the data_clustering
d <- dist(data_clustering, method="euclidean")

```

```{r}
#Here we use Ward's method to do grouping. The idea of this algorithm is to minimize the variance within a cluster. When finishing grouping, we use plot function to display our cluster dendrogram. As we want to divide the data into 6 groups, I use function rect.hclust, which will draw k=4 rectangles on the dendrogram to show the grouping.
pfit <- hclust(d, method="ward.D2")
par(mar = c(0, 0, 0, 0))
plot(pfit,labels=data_clustering_groupby_PF$Police_Force,main="Cluster Dendrogram")
rect.hclust(pfit, k=6)

```

```{r}

#Here we use cutree function to return the cluster each data point belong to. Here we want 6 groups.
groups <- cutree(pfit, k=6)

#this function is used to combine the columns we want from the data frame and the cluster it belong to and create a new data frame.
print_clusters <- function(df, groups, cols_to_print) {
  Ngroups <- max(groups)
  for (i in 1:Ngroups) {
    print(paste("cluster", i))
    print(df[groups == i, cols_to_print])
  }
}

cols_to_print <- c("Police_Force","Mean_Number_of_Casualties","Mean_Number_of_Vehicles","Mean_Accident_Severity")
print_clusters(data_clustering_groupby_PF, groups, cols_to_print)
```

VIsualising
```{r}
#To visualize the cluster, we need to project our data point onto a 2D plane, for which we need to use a dimension reduction method called Principal Component Analysis. This method will calculate the principal components which can reserve the variation and  sort them in a descending order using the degree of the reservation. Here we use the first 2 principal components to map our datapoint to a 2D plane.
princ <- prcomp(data_clustering)
nComp <-2
project2D <- as.data.frame(predict(princ, newdata=data_clustering)[,1:nComp])
hclust.project2D <- cbind(project2D, cluster=as.factor(groups), Police_Force=data_clustering_groupby_PF$Police_Force)
head(hclust.project2D)
```
```{r}
library('grDevices')

#This function is used to draw a convex hull using a group of data point. proj2Ddf is the datapoint which is now in 2D, groups is a vector of the cluster each point belong to.
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,lapply(unique(groups),
                       FUN = function(c) {
                         f <- subset(proj2Ddf, cluster==c);f[chull(f),]
                         }
                       )
  )
  }
hclust.hull <- find_convex_hull(hclust.project2D, groups)

```

```{r}
library(ggplot2)

#Here we use ggplot to draw the convex hull and place those data points in it. What's more, to show those data points, the convex hull will be translunce and the alpha is 0.4.
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster)) +
  geom_text(aes(label=c(1,nrow(princ)), color=cluster), hjust=0, vjust=1, size=3) +
  geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
  alpha=0.4, linetype=0) + theme(text=element_text(size=20))

```

```{r}

#We use clusterboot to check the stability of the cluster. The idea of it is that it will resample from the data frame and form some new clusters and see the similarity of the new clusters and the old ones. If the similarity is high, it means the cluster is highly stable.
#Here, we input our dataframe data_clustering, tell the function the grouping method we use is Ward's and the function need to create 6 cluster in each iteration. It will run 100 iterations and the result will be recorded in cboot.hclust. When getting the result, we do a summary.
library(fpc)
kbest.p <- 6
cboot.hclust <- clusterboot(data_clustering, clustermethod=hclustCBI,method="ward.D2", k=kbest.p)

summary(cboot.hclust$result)

```

```{r}
#Calculate the stability of each cluster.
1 - cboot.hclust$bootbrd/100
```

We can see that cluster 1,5 are highly stable, 3,6 are stable, and cluster 4 is not good enough which can seen on the cluster visualization as well.
```{r}
#Function for finding the number of good cluster

# This function will return the distance square of two data point.
sqr_euDist <- function(x, y) {
sum((x - y)^2)
}
# Function to calculate WSS of a cluster, represented as a n-by-d matrix
# (where n and d are the numbers of rows and columns of the matrix)
# which contains only points of the cluster.
wss <- function(clustermat) {
c0 <- colMeans(clustermat)
sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
# Function to calculate the total WSS. Argument `scaled_df`: data frame
# with normalised numerical columns. Argument `labels`: vector containing
# the cluster ID (starting at 1) for each row of the data frame.
wss_total <- function(scaled_df, labels) {
              wss.sum <- 0
              k <- length(unique(labels))
              for (i in 1:k)
                wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
              wss.sum
            }


# Function to calculate total sum of squared (TSS) distance of data
# points about the (global) mean. This is the same as WSS when the
# number of clusters (k) is 1.
tss <- function(scaled_df) {
          wss(scaled_df)
        }
# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
# for a vector of k values ranging from 1 to kmax.
CH_index <- function(scaled_df, kmax, method="kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
  stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
# wss.value[1] stores the WSS value for k=1 (when all the
# data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  if (method == "kmeans") {
# kmeans
    for (k in 2:kmax) {
      clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
      wss.value[k] <- clustering$tot.withinss
      }
    } 
  else {
    # hclust
    d <- dist(scaled_df, method="euclidean")
    pfit <- hclust(d, method="ward.D2")
    for (k in 2:kmax) {
      labels <- cutree(pfit, k=k)
      wss.value[k] <- wss_total(scaled_df, labels)
    }
    }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax-1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

```{r}
library(gridExtra)
# calculate the CH criterion
crit.df <- CH_index(data_clustering, 10, method="hclust")
fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)


```
We can see that CH criterion is maximized at k = 2,with another local maximum at k = 8
Aftering trying k=8, we found there would be one cluster is ustable, for which we use k=2 here.

```{r}
pfit <- hclust(d, method="ward.D2") 
plot(pfit, labels=data_clustering_groupby_PF$Police_Force, main="Cluster Dendrogram")
rect.hclust(pfit, k=2)
groups_hierarcy <- cutree(pfit, k=2)
cols_to_print <- c("Police_Force","Mean_Number_of_Casualties","Mean_Number_of_Vehicles","Mean_Accident_Severity")
print_clusters(data_clustering_groupby_PF, groups_hierarcy, cols_to_print)
```
```{r}
kbest.p <- 2
cboot.hclust <- clusterboot(data_clustering, clustermethod=hclustCBI,method="ward.D2", k=kbest.p)

summary(cboot.hclust$result)
1 - cboot.hclust$bootbrd/100

```

We can see that, these two clusters are highly stable.


Method 2: Kmeans
We use Kmeans to verify whether 2 cluster is reasonable.
```{r}
#We start by forming 5 clusters, 100 starts, and 100 iterations.
kbest.p <- 5
kmClusters <- kmeans(data_clustering, kbest.p, nstart=100, iter.max=100)
kmClusters$centers
```
```{r}
groups <- kmClusters$cluster
print_clusters(data_clustering_groupby_PF, groups, cols_to_print )
```


```{r}

kmClustering.ch <- kmeansruns(data_clustering, krange=1:10, criterion="ch")
kmClustering.ch$bestk
kmClustering.asw <- kmeansruns(data_clustering, krange=1:10, criterion="asw")
kmClustering.asw$bestk


```
```{r}
hclusting <- CH_index(data_clustering, 10, method="hclust")
print(hclusting$CH_index)
```


```{r}
library(gridExtra)
kmCritframe <- data.frame(k=1:10, ch=kmClustering.ch$crit,
asw=kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x=k, y=ch)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(kmCritframe, aes(x=k, y=asw)) +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="ASW") + theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)

```
2 is best
```{r}
fig <- c()
kvalues <- seq(2,5)
for (k in kvalues) {
  groups <- kmeans(data_clustering, k, nstart=100, iter.max=100)$cluster
  kmclust.project2D <- cbind(project2D, cluster=as.factor(groups),Police_Force=data_clustering_groupby_PF$Police_Force)
  kmclust.hull <- find_convex_hull(kmclust.project2D, groups)
  assign(paste0("fig", k),
    ggplot(kmclust.project2D, aes(x=PC1, y=PC2)) +
    geom_point(aes(shape=cluster, color=cluster)) +
    geom_polygon(data=kmclust.hull, aes(group=cluster, fill=cluster),
                  alpha=0.4, linetype=0) +
    labs(title = sprintf("k = %d", k)) +
    theme(legend.position="none", text=element_text(size=20))
    )
}

library(gridExtra)
grid.arrange(fig2, fig3, fig4, fig5, nrow=2)
```
```{r}
kbest.p <- 2
kmClusters <- kmeans(data_clustering, kbest.p, nstart=100, iter.max=100)
groups_kmeans <- kmClusters$cluster
print_clusters(data_clustering_groupby_PF, groups_kmeans, cols_to_print )

```
After using two method, we can see that 2 is the best number of cluster. In these two clusers, we can see that one cluster owns less points but more serious accidents(called cluster A)while the other one owns more points and slight accidents (named cluster B). Therefore, we can do some staff movement from the cluster B to cluster A. For example, transfer some police constables from Devon and Cornwall to Tayside. For another, we can see that  the mean number of casualties of data points in cluster A is generally lower than that in cluster B.It reveals that passengers are not a main cause for the accidents. The police organization in the cluster A should educate the public to drive safely and contact the relevant departments to fix some flaws on the roads. 

### Conclusion
After detailed data cleaning, data analysis, modeling, and clustering, we filtered out the first four categories of data that most affected the outcome of an accident through the decision tree, and found that Random_Forest or SVM was the ideal predictive model. After clustering, After using two methods, we can see that 2 is the best number of the cluster.





